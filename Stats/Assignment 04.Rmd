---
title: "Assignment-04"
author: "Sumanth Donthula"
date: "2022-10-22"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Question 1)

1.a)

X1 is almost Normally Distributed

X2 is Right Skewed

X3 is Randomly distributed

```{r}
Data1=read.table("As4Q1.txt", header = FALSE, sep = "")
colnames(Data1)=c("Y","X1","X2","X3")
Y=Data1$Y
X1=Data1$X1
X2=Data1$X2
X3=Data1$X3

dotchart(X1, main="Dot Plot for X1",xlab="Emptying Rate of Blood")
dotchart(X2, main="Dot Plot for X2",xlab="Ejection Rate of Blood")
dotchart(X3, main="Dot Plot for X3",xlab="Gas Measure")
```
1.b)

Y is correlated with X1 and X2 but not correlated with X3 from scatter plot.  

Based on correlation matrix, X1 and X2 have high colinearity. No correlation between X1 and X3 but X2 and X3 have less correlation between them. 

so from the aboove inferences multicolineaity exists.
```{r}

par(mfrow=c(3,2))
pairs(Data1)
cor(Data1[-1])

```
1.c)

The value of coeficient of X3 is 0.07 which seems less significant.

```{r}
Model1=lm(Y~X1+X2+X3)
Model1
summary(Model1)
```
Question 2)

2.a)

The three best hierarchical subset regression models are

```{r}
library(leaps)
Subs = regsubsets(Y~X1 + X2 + X3 + I(X1^2)+I(X2^2) + I(X3^2) + I(X1*X2) + I(X1*X3)+ I(X2*X3), method = 'exhaustive', data=Data1)

SubsDf = data.frame(features = summary(Subs)$which, adjr2 = summary(Subs)$adjr2)

colnames(SubsDf) = c('Intercept', 'X1', 'X2', 'X3', 'I(X1ˆ2)', 'I(X2ˆ2)', 'I(X3ˆ2)', 'I(X1 * X2)', 'I(X1 * X3)', 'I(X2 * X3)', 'AdjR_2')

SubsDf = SubsDf[order(-SubsDf$AdjR_2),][1:3,]
SubsDf
```
2.b)

There is no much difference in R Adjusted squared values.

Question 3)

3.a)

The regression model is $Y=1.023+0.965*X1+0.629*X2+0.676*X3$

```{r}
Data2=read.table("As4Q2.txt", header = FALSE, sep = "")
colnames(Data2)=c("Y","X1","X2","X3");
Y=Data2$Y
X1=Data2$X1
X2=Data2$X2
X3=Data2$X3

Mod2=lm(Y~X1+X2+X3)
Mod2

```
3.b)

Hypothesis test

H0: Beta1=Beta2=Beta3=0

Ha: At least one of the coeficient is not 0

Since the P value from the summary of model is 7.82e-12 which is less than 0.05 we reject null hypothesis. So, there is regreession relation between sales and the predictors.

```{r}
summary(Mod2)
```
3.c)

From the summary of model

Hypothesis test

H0 : Betak = 0

Ha : Betak <> 0

Since the T values of all coeficeints are greater than Ttest(2.021) we note that the null hypothesis is false and all the coeficeints are significant.

The conclusions of this test does not correspond to the one obtained in part (b).

```{r}
summary(Mod2)
t_ratio <- qt(0.975, nrow(Data2) - 4)
t_ratio
```
3.d)

The correlation matrix is as follows

```{r}
cor(Data2[-1])
```
3.e)

From b,c and d we found that there is correlation in data and increase in X1 by 1000 wont be a good thing keeping other predictors constant.

Question 4)

4.a)

From the plots we observe that the residues are spread over zero line with some variance and it can also be seen there are some outliers which are at extreme values of X.

```{r}
Y=Data1$Y
X1=Data1$X1
X2=Data1$X2
X3=Data1$X3

Model3=lm(Y~X1+X2+X1*X2)

par(mfrow=c(3,2))

plot(X1,Model1$residuals)
abline(0,0)
plot(X2,Model1$residuals)
abline(0,0)
plot(X1*X2,Model1$residuals)
abline(0,0)

```
4.b)
The residuals seems to be normally distributed.

```{r}

qqnorm(Model3$residuals)
qqline(Model3$residuals, col = "red")


```
4.c)

since vif of all predictors are >1 there will be multicolinearity.

```{r}
library(car)

vif(Model3)


```
4.d) 
Hypothesis test:

H0 : Observation is an Outlier

Ha : Observation is not an Outlier

We find that no observations are outliers with a level of significance of alpha = 0.05

```{r}
library(olsrr)
Dr =ols_plot_resid_stud_fit(model = Model3)

Dr = Dr$data[,'dsr']
Ttest= qt(1-0.05/(2*nrow(Data2)),25-3)
Dr[Dr>Ttest]


```
4.e)

The observations below has high leverage and values are greater than 2xmean(Hmat) and are outliers. The results are consistent with 9.13 a. Because these values are located far on dot plot and have large values of X1 and X2.

```{r}
Hmat <- influence(Model3)
Hmat$hat[(Hmat$hat > mean(Hmat$hat)*2)]

```
4.f)

Conclusion:

Dffits:

7 and 8 have high Dffits greater than 1 which says they are more influential.

Dfbeta:

7 and 8 have high Dfbetas greater than 1 which says they are more influential.

Cooks Distance:

The percentile value for 8 is significant in cooks distance which is more influetial.

```{r}
library(car)

dffits(Model3)[c(3,7,8,15)]

dfbetas(Model3)[c(3,7,8,15),]

dfbetas(Model3)[c(3,7,8,15),]

pf(cooks.distance(Model3)[c(3,7,8,15)],4,nrow(Data2)-4 )


```
